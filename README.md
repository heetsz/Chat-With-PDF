# Chat-With-PDF

Chat with your PDFs locally. Upload a PDF, it’s chunked and embedded with free local embeddings (HuggingFace), indexed with FAISS, and queried via semantic search. Answers are generated by Google Gemini using only the most relevant PDF chunks as context.

## Features
- Free local embeddings via `sentence-transformers/all-MiniLM-L6-v2`
- Fast vector search with FAISS (stored on disk)
- PDF parsing with `PyPDFLoader`
- REST API with FastAPI + CORS
- React + Vite frontend with simple upload and chat flow

## Tech Stack
- Frontend: React, Vite, Tailwind
- Backend: FastAPI, Uvicorn
- Retrieval: LangChain, FAISS, HuggingFace Embeddings
- Generation: Google Gemini (via `langchain-google-genai`)

## Project Structure

```
Client/
	package.json
	vite.config.js
	src/
		pages/
			Upload.jsx
			Chatbot.jsx
		lib/
			api.js
			upload_pdf.js

Server/
	main.py
	requirements.txt
	services/
		processing.py
	uploads/           # uploaded PDFs (gitignored in practice)
	faiss_index/       # FAISS index saved locally
```

## Prerequisites
- Python 3.10+
- Node.js 18+ (20+ recommended)
- A Google API key for Gemini (Model: `gemini-2.5-flash`)

## Environment Variables

Create a `.env` file in each app folder.

Server/.env
```
ALLOWED_ORIGINS=http://localhost:5173
GOOGLE_API_KEY=your_google_api_key_here
```

Client/.env
```
VITE_API_URL=http://localhost:8000
```

Notes:
- `ALLOWED_ORIGINS` is a comma-separated list used by CORS.
- `GOOGLE_API_KEY` is required for Gemini responses in chat. Embeddings are local and do not require API keys.
- `VITE_API_URL` points the frontend to your FastAPI server.

## Setup & Run

Backend (FastAPI)
```bash
cd Server
python -m venv .venv
. .venv/Scripts/activate   # Windows PowerShell: . .venv/Scripts/Activate.ps1
pip install -r requirements.txt
uvicorn main:app --reload --host 0.0.0.0 --port 8000
```

Frontend (Vite)
```bash
cd Client
npm install
npm run dev
```

Open the app at http://localhost:5173

## How It Works
1. Upload a PDF from the Upload page.
2. The server parses text with `PyPDFLoader`, chunks with `RecursiveCharacterTextSplitter`, creates embeddings using `HuggingFaceEmbeddings`, and stores vectors in a local FAISS index.
3. For chat, the server performs similarity search over FAISS to gather the most relevant chunks and sends them as context to Gemini (`gemini-2.5-flash`) for an answer.

## API Reference

Base URL: `http://localhost:8000`

- POST `/upload-pdf`
	- Form Data: `file` (PDF)
	- Response: `{ "message": "PDF uploaded and processed" }`

- POST `/chat`
	- JSON Body: `{ "question": "<your question>" }`
	- Response: `{ "answer": "<model answer>" }`

- GET `/`
	- Health check: `{ "success": "ChatWithPDF Server Running..." }`

## Key Files
- Frontend HTTP client: Client/src/lib/api.js (uses `VITE_API_URL`)
- Upload logic: Client/src/lib/upload_pdf.js
- Upload page: Client/src/pages/Upload.jsx
- Simple chat page: Client/src/pages/Chatbot.jsx
- FastAPI app: Server/main.py
- PDF processing & RAG: Server/services/processing.py

## Troubleshooting
- CORS blocked: ensure `ALLOWED_ORIGINS` includes your frontend origin (e.g., `http://localhost:5173`). Restart the server after changes.
- Gemini errors or empty responses: verify `GOOGLE_API_KEY` is set in `Server/.env` and the key has access to `gemini-2.5-flash`.
- “Index not found” or no results: upload and process at least one PDF to create the FAISS index in `Server/faiss_index/`.
- Large PDFs: initial processing can take longer; avoid very large files while testing.

## Development Scripts

Client
```bash
npm run dev       # start Vite dev server
npm run build     # production build
npm run preview   # preview production build
```

Server
```bash
uvicorn main:app --reload --host 0.0.0.0 --port 8000
```

## Privacy & Storage
- Uploaded PDFs are stored under `Server/uploads/`.
- Vector indexes are stored under `Server/faiss_index/`.
- Embeddings are generated locally with HuggingFace; only chat completion calls reach Google Gemini.

## Roadmap Ideas
- Chat UI with history and sources highlighting
- Multi-PDF or per-user indexes
- Docker compose for full stack
- Authentication and rate limiting
